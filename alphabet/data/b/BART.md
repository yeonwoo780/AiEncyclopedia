### BART (Bidirectional and Auto-Regressive Transformer)

---

BART는 [Transformer](../t/Transformer.md)를 기반으로 한 언어 모델 대량의 텍스트 데이터를 사용하여 사전 학습된 언어 모델

이 모델은 자연어 이해, 기계 번역, 요약, 질문 응답 등 다양한 NLP 태스크에서 높은 성능을 발휘.

#### BART의 두가지 주요 구성 요소

1. auto-regressive 인코더
   
   인코더는 문장의 각 단어를 하나씩 처리하며, 이전 단어에 대한 정보를 사용하여 다음 단어를 예측함
   
   그래서 이 모델은 문장 내의 의미론적인 특성과 구문적인 패턴을 학습할 수 있음
2. bi-directional 디코더
   
   이 디코더는 입력 문장을 순방향으로 처리하는 것 외에도 역방향으로 처리하여 모델이 문맥을 더 잘 파악하도록 함.
   
   BART는 임으로 mask를 적용하여 노이즈가 있는 입력 문장을 생성하고, 이를 모델에 입력하여 정상적인 문장을 생성하도록 학습.
   
   이에따라 주어진 입력 문장에 양방향 정보를 활용하여 좀 더 정확하게 문맥과 문장의 의미를 파악 할 수 있음




